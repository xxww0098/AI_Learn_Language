# ort 2.0.0-rc.10 ä¸­æ–‡ä½¿ç”¨æ•™ç¨‹

## æ¦‚è¿°

ort æ˜¯ ONNX Runtime çš„å®‰å…¨ Rust å°è£…åº“ï¼Œç”¨äºä¼˜åŒ–å’ŒåŠ é€Ÿæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¨ç†å’Œè®­ç»ƒã€‚å®ƒæä¾›äº†é«˜æ€§èƒ½çš„æœºå™¨å­¦ä¹ æ¨ç†èƒ½åŠ›ï¼Œæ”¯æŒå¤šç§ç¡¬ä»¶å¹³å°ã€‚

**ç‰ˆæœ¬**: 2.0.0-rc.10
**è®¸å¯è¯**: MIT OR Apache-2.0
**ä»“åº“**: https://github.com/pykeio/ort
**ä¸»é¡µ**: https://ort.pyke.io/

## ä¸»è¦ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½**: åŸºäº ONNX Runtime 1.22ï¼Œä¼˜åŒ–æ¨ç†é€Ÿåº¦
- ğŸ”’ **ç±»å‹å®‰å…¨**: æä¾›å®Œå…¨ç±»å‹å®‰å…¨çš„ Rust API
- ğŸ¯ **å¤šå¹³å°**: æ”¯æŒ CPUã€GPUã€å„ç§åŠ é€Ÿå™¨
- ğŸ› ï¸ **æ˜“ç”¨æ€§**: ç®€æ´çš„ API è®¾è®¡ï¼Œæ˜“äºé›†æˆ
- ğŸ“¦ **è½»é‡çº§**: æœ€å°åŒ–ä¾èµ–ï¼Œå¿«é€Ÿæ„å»º

## å®‰è£…

### æ·»åŠ ä¾èµ–

åœ¨ `Cargo.toml` ä¸­æ·»åŠ ï¼š

```toml
[dependencies]
ort = "2.0.0-rc.10"

# å¯é€‰ï¼šæ·»åŠ ç‰¹å®šåç«¯æ”¯æŒ
[dependencies.ort]
version = "2.0.0-rc.10"
features = ["cuda", "tensorrt", "openvino"]
```

### ç³»ç»Ÿè¦æ±‚

- ONNX Runtime åº“ï¼ˆè‡ªåŠ¨ä¸‹è½½æˆ–æ‰‹åŠ¨å®‰è£…ï¼‰
- æ”¯æŒçš„æ“ä½œç³»ç»Ÿï¼šWindowsã€Linuxã€macOS
- å¯é€‰ï¼šCUDAã€TensorRTã€OpenVINO ç­‰åŠ é€Ÿåº“

## åŸºæœ¬ç”¨æ³•

### 1. ç¯å¢ƒåˆå§‹åŒ–

```rust
use ort::{Environment, ExecutionProvider, GraphOptimizationLevel};

fn main() -> ort::Result<()> {
    // åˆå§‹åŒ– ONNX Runtime ç¯å¢ƒ
    let environment = Environment::builder()
        .with_name("MyApp")
        .with_log_level(ort::LoggingLevel::Warning)
        .build()?;

    // è®¾ç½®æ‰§è¡Œæä¾›ç¨‹åº
    let execution_providers = vec![
        ExecutionProvider::CUDA(Default::default()),
        ExecutionProvider::CPU(Default::default()),
    ];

    Ok(())
}
```

### 2. åŠ è½½æ¨¡å‹

```rust
use ort::{Session, SessionBuilder};

fn load_model() -> ort::Result<Session> {
    let session = SessionBuilder::new()?
        .with_optimization_level(GraphOptimizationLevel::All)?
        .with_intra_threads(4)?
        .commit_from_file("model.onnx")?;

    Ok(session)
}
```

### 3. åŸºæœ¬æ¨ç†

```rust
use ort::{Session, Value, inputs, ArrayExtensions};
use ndarray::Array;

fn run_inference(session: &Session) -> ort::Result<()> {
    // å‡†å¤‡è¾“å…¥æ•°æ®
    let input_data = Array::from_shape_vec(
        (1, 3, 224, 224),
        vec![0.0f32; 1 * 3 * 224 * 224]
    ).unwrap();

    // åˆ›å»ºè¾“å…¥å¼ é‡
    let input_tensor = Value::from_array(input_data)?;

    // æ‰§è¡Œæ¨ç†
    let outputs = session.run(inputs![input_tensor]?)?;

    // è·å–è¾“å‡º
    let output: &Value = &outputs[0];
    let output_array = output.try_extract_tensor::<f32>()?;

    println!("è¾“å‡ºå½¢çŠ¶: {:?}", output_array.shape());
    println!("è¾“å‡ºæ•°æ®: {:?}", output_array.slice(s![0, 0..5]));

    Ok(())
}
```

## é«˜çº§åŠŸèƒ½

### 1. å›¾åƒåˆ†ç±»ç¤ºä¾‹

```rust
use ort::{Session, Value, inputs};
use ndarray::{Array4, s};
use image::ImageBuffer;

struct ImageClassifier {
    session: Session,
    input_shape: (usize, usize, usize, usize), // (batch, channels, height, width)
}

impl ImageClassifier {
    fn new(model_path: &str) -> ort::Result<Self> {
        let session = SessionBuilder::new()?
            .with_optimization_level(GraphOptimizationLevel::All)?
            .commit_from_file(model_path)?;

        Ok(Self {
            session,
            input_shape: (1, 3, 224, 224),
        })
    }

    fn preprocess_image(&self, image: &ImageBuffer<image::Rgb<u8>, Vec<u8>>) -> Array4<f32> {
        let (width, height) = image.dimensions();
        let mut input = Array4::zeros(self.input_shape);

        for y in 0..height {
            for x in 0..width {
                let pixel = image.get_pixel(x, y);
                let r = pixel[0] as f32 / 255.0;
                let g = pixel[1] as f32 / 255.0;
                let b = pixel[2] as f32 / 255.0;

                input[[0, 0, y as usize, x as usize]] = (r - 0.485) / 0.229;
                input[[0, 1, y as usize, x as usize]] = (g - 0.456) / 0.224;
                input[[0, 2, y as usize, x as usize]] = (b - 0.406) / 0.225;
            }
        }

        input
    }

    fn predict(&self, image: &ImageBuffer<image::Rgb<u8>, Vec<u8>>) -> ort::Result<Vec<f32>> {
        let input_data = self.preprocess_image(image);
        let input_tensor = Value::from_array(input_data)?;

        let outputs = self.session.run(inputs![input_tensor]?)?;
        let output = outputs[0].try_extract_tensor::<f32>()?;

        Ok(output.slice(s![0, ..]).to_vec())
    }
}

fn main() -> ort::Result<()> {
    let classifier = ImageClassifier::new("resnet50.onnx")?;
    
    let img = image::open("test_image.jpg")
        .unwrap()
        .to_rgb8();
    
    let predictions = classifier.predict(&img)?;
    
    // æ‰¾åˆ°æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«
    let max_idx = predictions
        .iter()
        .enumerate()
        .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
        .map(|(idx, _)| idx)
        .unwrap();
    
    println!("é¢„æµ‹ç±»åˆ«: {}, æ¦‚ç‡: {:.4}", max_idx, predictions[max_idx]);
    
    Ok(())
}
```

### 2. æ–‡æœ¬å¤„ç†æ¨¡å‹

```rust
use ort::{Session, Value, inputs};
use ndarray::Array2;

struct TextProcessor {
    session: Session,
    vocab_size: usize,
    max_length: usize,
}

impl TextProcessor {
    fn new(model_path: &str, vocab_size: usize, max_length: usize) -> ort::Result<Self> {
        let session = SessionBuilder::new()?
            .with_optimization_level(GraphOptimizationLevel::All)?
            .commit_from_file(model_path)?;

        Ok(Self {
            session,
            vocab_size,
            max_length,
        })
    }

    fn tokenize(&self, text: &str) -> Vec<i64> {
        // ç®€å•çš„è¯æ±‡è¡¨æ˜ å°„ï¼ˆå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨ä¸“é—¨çš„åˆ†è¯å™¨ï¼‰
        text.chars()
            .map(|c| c as i64)
            .take(self.max_length)
            .collect()
    }

    fn pad_sequence(&self, tokens: Vec<i64>) -> Array2<i64> {
        let mut padded = vec![0i64; self.max_length];
        for (i, &token) in tokens.iter().enumerate() {
            if i < self.max_length {
                padded[i] = token;
            }
        }
        Array2::from_shape_vec((1, self.max_length), padded).unwrap()
    }

    fn process_text(&self, text: &str) -> ort::Result<Vec<f32>> {
        let tokens = self.tokenize(text);
        let input_ids = self.pad_sequence(tokens);
        
        let input_tensor = Value::from_array(input_ids)?;
        let outputs = self.session.run(inputs![input_tensor]?)?;
        
        let output = outputs[0].try_extract_tensor::<f32>()?;
        Ok(output.slice(s![0, ..]).to_vec())
    }
}
```

### 3. æ‰¹é‡æ¨ç†

```rust
use ort::{Session, Value, inputs};
use ndarray::Array4;

fn batch_inference(session: &Session, batch_data: Vec<Array4<f32>>) -> ort::Result<Vec<Vec<f32>>> {
    let mut results = Vec::new();
    
    for data in batch_data {
        let input_tensor = Value::from_array(data)?;
        let outputs = session.run(inputs![input_tensor]?)?;
        let output = outputs[0].try_extract_tensor::<f32>()?;
        results.push(output.slice(s![0, ..]).to_vec());
    }
    
    Ok(results)
}

// å¼‚æ­¥æ‰¹é‡æ¨ç†
use tokio;

async fn async_batch_inference(
    session: &Session,
    batch_data: Vec<Array4<f32>>
) -> ort::Result<Vec<Vec<f32>>> {
    let tasks: Vec<_> = batch_data.into_iter().map(|data| {
        let session = session.clone(); // æ³¨æ„ï¼šSession éœ€è¦æ”¯æŒ Clone
        tokio::spawn(async move {
            let input_tensor = Value::from_array(data)?;
            let outputs = session.run(inputs![input_tensor]?)?;
            let output = outputs[0].try_extract_tensor::<f32>()?;
            Ok::<Vec<f32>, ort::Error>(output.slice(s![0, ..]).to_vec())
        })
    }).collect();

    let mut results = Vec::new();
    for task in tasks {
        results.push(task.await.unwrap()?);
    }
    
    Ok(results)
}
```

## æ€§èƒ½ä¼˜åŒ–

### 1. æ‰§è¡Œæä¾›ç¨‹åºé…ç½®

```rust
use ort::{ExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider};

fn configure_providers() -> Vec<ExecutionProvider> {
    vec![
        // GPU åŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        ExecutionProvider::CUDA(CUDAExecutionProvider::default()
            .with_device_id(0)
            .with_gpu_mem_limit(2 * 1024 * 1024 * 1024) // 2GB
        ),
        
        // CPU åå¤‡
        ExecutionProvider::CPU(CPUExecutionProvider::default()
            .with_intra_op_num_threads(4)
            .with_inter_op_num_threads(2)
        ),
        
        // å…¶ä»–åŠ é€Ÿå™¨
        ExecutionProvider::TensorRT(Default::default()),
        ExecutionProvider::OpenVINO(Default::default()),
    ]
}
```

### 2. å†…å­˜ç®¡ç†

```rust
use ort::{AllocatorType, MemType};

fn optimize_memory_usage(session_builder: SessionBuilder) -> ort::Result<Session> {
    session_builder
        .with_memory_pattern(true)?
        .with_allocator(AllocatorType::Arena)?
        .with_memory_type(MemType::CPUInput)?
        .commit_from_file("model.onnx")
}
```

### 3. å›¾ä¼˜åŒ–

```rust
use ort::GraphOptimizationLevel;

fn optimize_graph(session_builder: SessionBuilder) -> ort::Result<Session> {
    session_builder
        .with_optimization_level(GraphOptimizationLevel::All)?
        .with_optimized_model_file_path("optimized_model.onnx")?
        .commit_from_file("model.onnx")
}
```

## é”™è¯¯å¤„ç†

### 1. è‡ªå®šä¹‰é”™è¯¯ç±»å‹

```rust
use ort::Error as OrtError;

#[derive(Debug)]
pub enum MyError {
    OrtError(OrtError),
    InvalidInput(String),
    ModelNotFound,
}

impl From<OrtError> for MyError {
    fn from(error: OrtError) -> Self {
        MyError::OrtError(error)
    }
}

fn safe_inference(session: &Session, input: Array4<f32>) -> Result<Vec<f32>, MyError> {
    if input.shape()[0] != 1 {
        return Err(MyError::InvalidInput("æ‰¹é‡å¤§å°å¿…é¡»ä¸º1".to_string()));
    }

    let input_tensor = Value::from_array(input)?;
    let outputs = session.run(inputs![input_tensor]?)?;
    let output = outputs[0].try_extract_tensor::<f32>()?;
    
    Ok(output.slice(s![0, ..]).to_vec())
}
```

### 2. é‡è¯•æœºåˆ¶

```rust
use std::thread::sleep;
use std::time::Duration;

fn retry_inference<F, T>(mut f: F, max_retries: u32) -> Result<T, OrtError>
where
    F: FnMut() -> Result<T, OrtError>,
{
    let mut retries = 0;
    loop {
        match f() {
            Ok(result) => return Ok(result),
            Err(err) => {
                if retries >= max_retries {
                    return Err(err);
                }
                retries += 1;
                sleep(Duration::from_millis(100 * retries as u64));
            }
        }
    }
}
```

## æ¨¡å‹è½¬æ¢å’Œéƒ¨ç½²

### 1. æ¨¡å‹æ ¼å¼è½¬æ¢

```rust
// ä» PyTorch æ¨¡å‹è½¬æ¢
fn convert_pytorch_model() {
    // åœ¨ Python ä¸­æ‰§è¡Œè½¬æ¢
    // torch.onnx.export(model, dummy_input, "model.onnx")
}

// æ¨¡å‹é‡åŒ–
fn quantize_model() -> ort::Result<()> {
    // ä½¿ç”¨ ONNX Runtime é‡åŒ–å·¥å…·
    // è¿™é€šå¸¸åœ¨ Python ä¸­å®Œæˆï¼Œç„¶ååœ¨ Rust ä¸­åŠ è½½é‡åŒ–æ¨¡å‹
    let session = SessionBuilder::new()?
        .commit_from_file("quantized_model.onnx")?;
    
    Ok(())
}
```

### 2. æ¨¡å‹éªŒè¯

```rust
fn validate_model(model_path: &str) -> ort::Result<()> {
    let session = SessionBuilder::new()?
        .commit_from_file(model_path)?;
    
    // æ£€æŸ¥è¾“å…¥/è¾“å‡ºä¿¡æ¯
    let input_info = session.inputs();
    let output_info = session.outputs();
    
    println!("æ¨¡å‹è¾“å…¥æ•°é‡: {}", input_info.len());
    println!("æ¨¡å‹è¾“å‡ºæ•°é‡: {}", output_info.len());
    
    for (i, input) in input_info.iter().enumerate() {
        println!("è¾“å…¥ {}: {:?}", i, input);
    }
    
    for (i, output) in output_info.iter().enumerate() {
        println!("è¾“å‡º {}: {:?}", i, output);
    }
    
    Ok(())
}
```

## æœ€ä½³å®è·µ

1. **æ¨¡å‹é¢„åŠ è½½**: åœ¨åº”ç”¨å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹ï¼Œé¿å…è¿è¡Œæ—¶å»¶è¿Ÿ
2. **æ‰¹é‡å¤„ç†**: å°½å¯èƒ½ä½¿ç”¨æ‰¹é‡æ¨ç†æé«˜ååé‡
3. **å†…å­˜ç®¡ç†**: åˆç†é…ç½®å†…å­˜ä½¿ç”¨ï¼Œé¿å…å†…å­˜æ³„æ¼
4. **é”™è¯¯å¤„ç†**: å®ç°å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
5. **æ€§èƒ½ç›‘æ§**: ç›‘æ§æ¨ç†æ—¶é—´å’Œèµ„æºä½¿ç”¨æƒ…å†µ

## å¸¸è§é—®é¢˜

### 1. æ¨¡å‹åŠ è½½å¤±è´¥

```rust
fn diagnose_model_loading(model_path: &str) {
    match SessionBuilder::new() {
        Ok(builder) => {
            match builder.commit_from_file(model_path) {
                Ok(_) => println!("æ¨¡å‹åŠ è½½æˆåŠŸ"),
                Err(e) => println!("æ¨¡å‹åŠ è½½å¤±è´¥: {:?}", e),
            }
        }
        Err(e) => println!("åˆ›å»ºä¼šè¯æ„å»ºå™¨å¤±è´¥: {:?}", e),
    }
}
```

### 2. æ€§èƒ½ä¼˜åŒ–

- é€‰æ‹©åˆé€‚çš„æ‰§è¡Œæä¾›ç¨‹åº
- è°ƒæ•´çº¿ç¨‹æ•°é‡
- å¯ç”¨å›¾ä¼˜åŒ–
- ä½¿ç”¨æ¨¡å‹é‡åŒ–

### 3. å†…å­˜ä¸è¶³

- å‡å°‘æ‰¹é‡å¤§å°
- å¯ç”¨å†…å­˜æ¨¡å¼
- ä½¿ç”¨æµå¼å¤„ç†

## æ€»ç»“

ort æä¾›äº†ä¸€ä¸ªå¼ºå¤§ä¸”é«˜æ•ˆçš„ ONNX Runtime Rust æ¥å£ï¼Œé€‚ç”¨äºå„ç§æœºå™¨å­¦ä¹ æ¨ç†åœºæ™¯ã€‚é€šè¿‡åˆç†çš„é…ç½®å’Œä¼˜åŒ–ï¼Œå¯ä»¥å®ç°é«˜æ€§èƒ½çš„ AI åº”ç”¨éƒ¨ç½²ã€‚

æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒï¼š
- [ort å®˜æ–¹æ–‡æ¡£](https://ort.pyke.io/)
- [ONNX Runtime æ–‡æ¡£](https://onnxruntime.ai/)
- [GitHub ä»“åº“](https://github.com/pykeio/ort)